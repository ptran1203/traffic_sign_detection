{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "traffic_signs_detection",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ptran1203/traffic_sign_detection/blob/main/traffic_signs_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3lSfY0bzSdn6"
      },
      "source": [
        "## If you need tfrecord data format, you may want to do this step\n",
        "\n",
        "Add path for [this folder](https://drive.google.com/drive/folders/1msDocIGPo34Xe8JgVYWb0efMzETMTrL6?usp=sharing) to your google drive, It should located at top level of your drive *directory*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dmZPz0SeR_P7"
      },
      "source": [
        "from google.colab import drive, output\n",
        "drive.mount('/content/drive')\n",
        "# !pip install tf-models-nightly\n",
        "ZALO_DIR = \"/content/drive/My Drive/zalo_ai/\"\n",
        " \n",
        "%cd \"/content\"\n",
        "!rm -rf traffic_sign_detection\n",
        "!git clone https://github.com/ptran1203/traffic_sign_detection\n",
        "%cd traffic_sign_detection\n",
        "output.clear()"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is8QDBMHrxSA"
      },
      "source": [
        "## Copy the file you need for training or inference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h_pcGApSRMyp"
      },
      "source": [
        "!cp \"/content/drive/My Drive/zalo_ai/images.tfrecords\" /content/traffic_sign_detection/.\n",
        "# !cp \"/content/drive/My Drive/zalo_ai/images_best_preds.tfrecords\" /content/traffic_sign_detection/.\n",
        "!cp \"/content/drive/My Drive/zalo_ai/images_test.tfrecords\" /content/traffic_sign_detection/.\n",
        "# !cp \"/content/drive/My Drive/zalo_ai/images_private_test.tfrecords\" /content/traffic_sign_detection/.\n",
        "# !cp \"/content/drive/MyDrive/zalo_ai/weight_dense.h5\" \"/content/traffic_sign_detection/weight_dense.h5\"\n",
        "ZALO_DIR = \"/content/traffic_sign_detection/\""
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2AfLenFTya5",
        "outputId": "fd22d890-4012-4eff-8389-d3e911f03be3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import json\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import model as m\n",
        "import data_processing\n",
        "import losses\n",
        "import utils\n",
        "import math\n",
        "import datetime\n",
        "from prediction import Prediction\n",
        "import os\n",
        "from collections import Counter\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "DATA_DIR = \"/content/drive/My Drive/zalo_ai/za_traffic_2020/za_traffic_2020/\"\n",
        "TFRECORDS_FILE = ZALO_DIR + 'images.tfrecords'\n",
        "TFRECORDS_FILE_PRED = \"/content/drive/MyDrive/zalo_ai/images_best_preds.tfrecords\" #ZALO_DIR + 'images_best_preds.tfrecords'\n",
        "TFRECORDS_FILE_TEST = ZALO_DIR + 'images_test.tfrecords'\n",
        "WEIGHT_FILE = ZALO_DIR + 'weight_dense.h5'\n",
        "train_dir = DATA_DIR + \"traffic_train/\"\n",
        "test_dir = DATA_DIR + \"traffic_public_test/\"\n",
        "json_path = ZALO_DIR + \"train_traffic_sign_dataset.json\"\n",
        "label_map = {\n",
        "    1: \"No entry\",\n",
        "    2: \"No parking / waiting\",\n",
        "    3: \"No turning\",\n",
        "    4: \"Max Speed\",\n",
        "    5: \"Other prohibition signs\",\n",
        "    6: \"Warning\",\n",
        "    7: \"Mandatory\",\n",
        "}\n",
        "\n",
        "WIDTH = 1622\n",
        "HEIGHT = 626\n",
        "\n",
        "metadata = json.load(open(json_path, \"r\"))\n",
        "test_file_id = \"1227,106,7543,7876,7175,6623,877,1169,10568,219,1572,639,12108,10186,9343,11598,377,740,12044,405,6497,12123,451,6725,10990,1062,87,12076,207,7923,232,309,913,7565,10776,11387,9447,1978,11185,9973,9117,10054,8066,8380,10048,8623,6560,1716,9672,12160,1242,12396,512,10060,275,12231,8665,150,10482,255,330,722,6864,10708,374,11104,545,8079,779,7145,6646,542,1739,11141,12334,290,961,10300,11653,501,244,10108,8560,8141,9670,8588,7852,248,10673,8298,1265,632,8546,11878,11260,9472,10333,554,8069,64,749,12290,7854,7903,11163,7898,1478,10397,12407,12504,10908,10431,12464,222,6442,745,10102,421,8547,12071,824,6552,197,482,10122,30,9819,10841,367,9928,878,12302,694,9411,11854,7907,8217,6801,10586,495,420,6350,11876,9093,12257,676,137,12297,12046,8589,882,240,11240,1268,609,12133,531,318,7340,8252,879,9061,414,958,10469,398,677,11381,10954,6846,145,734,12467,7988,725,11966,590,952,10271,52,202,9437,1879,93,10110,407,10437,8320,9044,1985,7653,10943,278,871,12242,9893,118,691,12500,12251,6152,8307,315,83,7856,8865,8569,6545,12184,777,48,479,742,94,6316,6618,9542,11855,7759,7307,7011,7202,114,19,324,801,12299,10249,783,6795,319,8212,7192,9671,12062,12253,103,9290,9701,12121,612,573,419,7329,306,7493,995,791,10981,973,12027,6893,11137,461,288,774,8289,690,11226,7050,12341,9355,863,6901,7849,9965,99,835,9493,7204,490,12243,11617,7844,7206,6165,10425,7156,506,12469,84,7909,10595,11883,154,737,6769,8684,4,12031,6480,964,12379,328,6625,6024,12282,843,1275,1251,7957,105,601,6059,873,9573,481,7542,6607,12069,12462,951,92,813,12455,915,12157,11047,11880,845,122,7512,46,498,9377,6274,53,924,647,502,1212,262,7668,214,11477,191,1839,341,12511,178,668,11450,10396,11177,97,657,345,11,7618,6492,12308,8124,351,10540,0,39,152,970,12202,6764,12204,10411,12132,773,12384,12206,1165,10383,237,1799,6155,1934,827,360,187,11199,23,8531,388,1649,11794,119,11684,6684,9698,10560,11249,12049,1731,12235,12238,11683,9565,7615,916,11771,7182,784,7481,10316,10867,116,470,841,7073,8186,582,6303,12126,10783,9921,866,10426,1013,9230,11827,1936,8337,947,11377,6139,43,603,11786,13,703,8426,714,11373,926,12311,981,9961,10819,12237,12195,11386,9329,10545,12395,10969,11943,11867,1774,1887,968,7786,648,1134,11356,6257,316,1538,11074,806,898,8603,1253,9902,7869,1829,984,6737,12409,12349,8792,8892,75,7484,12312,906,434,6302,10370,10974,12368,923,285,588,8303,249,9264,394,8783,8954,7317,11742,1280,787,12314,8128,907,746,6336,7250,303,199,850,380,11093,6614,6616,708,10013,6606,1488,10859,1100,6346,71,7951,12170,10279,883,1559,7998,6757,550,88,400,331,497,7647,12382,7430,7767,90,8592,1394,100,11209,11513,1050,9862,9165,6343,131,726,1205,6522,760,9282,1337,670,9963,10075,54,815,12322,6766,7127,473,689,174,10064,80,562,6167,10053,936,11752,143,8156,310,658,12410,7236,7633,6304,7819,1690,126,1555,12459,8226,830,127,10814,919,1035,12087,10882,1230,503,423,7060\"\n",
        "test_file_id = sorted([int(c) for c in test_file_id.split(\",\")])\n",
        "data_info = [\n",
        "    {\n",
        "        \"bbox\": [[0, 0, 0, 0]],\n",
        "        \"label\": [0],\n",
        "        \"id\": x,\n",
        "    } for x in test_file_id\n",
        "]\n",
        "\n",
        "if not os.path.isfile(TFRECORDS_FILE):\n",
        "    raise FileNotFoundError(f\"{TFRECORDS_FILE} does not exist\")\n",
        "fdataset = tf.data.TFRecordDataset(TFRECORDS_FILE)\n",
        "\n",
        "if os.path.isfile(TFRECORDS_FILE_PRED):\n",
        "    bdataset = tf.data.TFRecordDataset(TFRECORDS_FILE_PRED)\n",
        "    combined = bdataset.concatenate(fdataset)\n",
        "else:\n",
        "    print(f\"{TFRECORDS_FILE_PRED} does not exist\")\n",
        "    bdataset = None\n",
        "    combined = None"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/zalo_ai/images_best_preds.tfrecords does not exist\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kyv2023rUFsU"
      },
      "source": [
        "import IPython.display as display\n",
        "from data_processing import *\n",
        "autotune = tf.data.experimental.AUTOTUNE\n",
        "\n",
        "num_classes= 7\n",
        "batch_size= 2\n",
        "\n",
        "data_processor =data_processing.DataProcessing(400, 154)\n",
        "\n",
        "label_encoder = m.LabelEncoder()\n",
        "dataset = (combined or fdataset).map(data_processor.preprocess_data)\n",
        "dataset = dataset.shuffle(8 * batch_size)\n",
        "dataset = dataset.padded_batch(\n",
        "    batch_size,\n",
        "    padding_values=(0.0, 1e-8, tf.cast(-1, tf.int64)),\n",
        "    drop_remainder=True,\n",
        ")\n",
        "dataset = dataset.map(\n",
        "    label_encoder.encode_batch, num_parallel_calls=autotune\n",
        ")\n",
        "dataset = dataset.apply(tf.data.experimental.ignore_errors())\n",
        "dataset = dataset.prefetch(autotune)\n",
        "\n",
        "val_size = 0\n",
        "train_size = 4500 # + 1211 HARDCODE if using some test image (pseudo labeling)\n",
        "train_steps_per_epoch = train_size // batch_size\n",
        "train_steps = 4 * 10000\n",
        "epochs = train_steps // train_steps_per_epoch\n",
        "\n",
        "learning_rates = [2.5e-05, 0.000625, 0.00125, 0.0025, 0.00025, 2.5e-05]\n",
        "learning_rate_boundaries = [125, 250, 500, 240000, 360000]\n",
        "learning_rate_fn = tf.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    boundaries=learning_rate_boundaries, values=learning_rates\n",
        ")\n",
        "optimizer = tf.optimizers.SGD(learning_rate=learning_rate_fn, momentum=0.9)\n",
        "\n",
        "callbacks_list = [\n",
        "    tf.keras.callbacks.ModelCheckpoint(\n",
        "        filepath=\"/content/drive/MyDrive/weight_dense.h5\",\n",
        "        monitor=\"loss\",\n",
        "        save_best_only=False,\n",
        "        save_weights_only=True,\n",
        "        verbose=1,\n",
        "    )\n",
        "]\n",
        "\n",
        "backbone = \"densenet121\"\n",
        "WEIGHT_FILE = ZALO_DIR + \"weight_dense.h5\"\n",
        "label_smoothing = False\n",
        "\n",
        "model = m.RetinaNet(num_classes, backbone=backbone)\n",
        "model.compile(optimizer=optimizer, \n",
        "              loss=losses.RetinaNetLoss(num_classes, label_smoothing=label_smoothing))\n",
        "\n",
        "# Train 1 sample first so we can load the weight\n",
        "model.fit(dataset.take(1))\n",
        "utils.try_ignore_error(model.load_weights, WEIGHT_FILE)\n",
        "\n",
        "H = model.fit(dataset.repeat(),\n",
        "            epochs=epochs,\n",
        "            steps_per_epoch=train_steps_per_epoch,\n",
        "            callbacks=callbacks_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DoTVePVD_k5W"
      },
      "source": [
        "# bdataset = tf.data.TFRecordDataset(TFRECORDS_FILE_PRED)\n",
        "# btfdataset = bdataset.map(DataProcessing(convert_xywh=0).preprocess_data)\n",
        "for img, bbox, label in btfdataset.skip(100).take(5):\n",
        "    utils.visualize_detections(img ,bbox, label, label,\n",
        "                               (15,15))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mpxd2yPF8kEZ"
      },
      "source": [
        "## Inference model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hccSH0ni8mpF"
      },
      "source": [
        "import time\n",
        "image = tf.keras.Input(shape=[None, None, 3], name=\"image\")\n",
        "weight_path = \"/content/drive/MyDrive/weight_dense.h5\"\n",
        "model.fit(np.random.rand(1, 896, 2304, 3), np.random.rand(1, 386694, 5))\n",
        "model.load_weights(weight_path)\n",
        "predictions = model(image, training=False)\n",
        "detections = m.DecodePredictions(confidence_threshold=0.6,\n",
        "                                 num_classes=num_classes,\n",
        "                                 max_detections_per_class=10,\n",
        "                                 nms_iou_threshold=0.35,\n",
        "                                 verbose=0)(image, predictions)\n",
        "inference_model = tf.keras.Model(inputs=image, outputs=detections)\n",
        "ptest_dataset = tf.data.TFRecordDataset(\"images_private_test.tfrecords\")\n",
        "time.ctime(os.path.getmtime(weight_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kpIGBgOzmhsW"
      },
      "source": [
        "with open(\"private_id_list\", \"r\") as f:\n",
        "    ptest_id_list = f.read().split(\",\")\n",
        "    ptest_id_list = [int(x) for x in ptest_id_list]\n",
        "\n",
        "data_info = [\n",
        "        {\n",
        "            \"bbox\": [[0, 0, 0, 0]],\n",
        "            \"label\": [0],\n",
        "            \"id\": int(x),\n",
        "        } for x in test_file_id\n",
        "    ]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzpaR39P81pG"
      },
      "source": [
        "from prediction import Prediction as P\n",
        "import datetime\n",
        "\n",
        "test_dataset = tf.data.TFRecordDataset(\"images_test.tfrecords\")\n",
        "\n",
        "class Prediction(P):\n",
        "    pass\n",
        "\n",
        "skip = 123#ptest_id_list.index(134)\n",
        "take = 1\n",
        "predictor = Prediction(inference_model, 230, HEIGHT)\n",
        "# print(ptest_id_list[skip:skip+take])\n",
        "for sample in test_dataset.skip(skip).take(take):\n",
        "    start = datetime.datetime.now()\n",
        "    image, boxes, scores, classes = predictor.detect_single_image(sample, show=False)\n",
        "    print(\"Detect in {} - {} objects\".format(\n",
        "        datetime.datetime.now() - start,\n",
        "        len(boxes)\n",
        "    ))\n",
        "    class_names = [\n",
        "        label_map[int(x)] for x in classes\n",
        "    ]\n",
        "    utils.visualize_detections(\n",
        "        image,\n",
        "        boxes,\n",
        "        class_names,\n",
        "        scores,\n",
        "        figsize=(15,15),\n",
        "        color=[1,1,1]\n",
        "    )\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o3aGVGY9bavk"
      },
      "source": [
        "submission = []\n",
        "idx = 0\n",
        "import datetime\n",
        "start = datetime.datetime.now()\n",
        "for sample in test_dataset:\n",
        "    image, boxes, scores, classes = predictor.detect_single_image(sample)\n",
        "    if not isinstance(boxes, list):\n",
        "        boxes = boxes.numpy()\n",
        "        scores = scores.numpy()\n",
        "        classes = classes.numpy()\n",
        "\n",
        "    for i in range(len(boxes)):\n",
        "        box = boxes[i]\n",
        "        x1, y1, x2, y2 = box\n",
        "        xywh = [x1, y1, x2 - x1, y2 - y1]\n",
        "        # print(xywh, data_info[idx][\"id\"])\n",
        "        score = scores[i]\n",
        "        cls = classes[i]\n",
        "        submission.append({\n",
        "            \"image_id\": data_info[idx][\"id\"],\n",
        "            \"category_id\": int(cls),\n",
        "            \"bbox\": [float(z) for z in xywh],\n",
        "            \"score\": float(score),\n",
        "        })\n",
        "\n",
        "    utils._print_progress(\"{}/{}\".format(idx, 585))\n",
        "    idx += 1\n",
        "\n",
        "print(\"Predict in {}\".format(datetime.datetime.now() - start))\n",
        "with open(\"submission.json\", \"w\") as f:\n",
        "    json.dump(submission, f, indent=2)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
